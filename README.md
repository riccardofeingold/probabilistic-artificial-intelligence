# Probabilistic Artificial Intelligence 2023
## Task 0: Implementation of Baysian Interference
The goal of this simple exercise is to calculate the posterior probabilities of $P(H_i|X)$ for $i=1,2,3$ using Bayes theorem. According this formula, the probability of the hypothesis $H_i$ given $X$ can be calculated by multiplying the sum of the porbability of all $X$ given $H_i$ with the corresponding prior probability for the hypothetis $H_i$. All this is divided by the marginal likelihood.

## Task 1: Gaussian Process Regression
The goal of this task was to model air pollution measurements using Gaussian Process Regression and predict the pollution at new locations. To implement the Gaussian process regressor I used the Sklearn library. Since the priors have zero mean, I enabled the option to normalize the target values, which gets reverted before the GP prediction happens. In order to find a suitable kernel function I implemented a 3-fold cross validator that tested 5 different kernels. For each model, the asymmetric cost gets calculated. Based on the results of the cross-validation, the superposition of the Matern kernel with nu=1.5 and length_scale=0.054 and the white kernel with noise_level=0.00528 received the lowest average cost over all folds. The white kernel has been added due to the fact that the training data contains noise. I also tested one kernel without a white kernel for comparison. Other kernels I tested can be found inside the model_selection method and the corresponding results are mentioned as comments in the main method. To ensure that the predictions at areas with binary value 1 are not below the ground truth, I decided to set the predictions equal to the sum of the posterior standard deviation and the posterior mean for these areas. To manage the large dataset problem, I used the train_test_split method from Sklearn to randomly sample 70% of the data as the training set. This ensures a manageable computational load without compromising the richness of the data.

## Task 2: Approximate Baysian Inference in Neural Networks via SWA-Gaussian
The goal of this task is to fit an approximate Gaussian posterior over the weights of the neural network using SWAG. With this approach, a more calibrated model can be achieved whose confidence aligns more with what its actual performance is. First, I implemented the SWAG diagonal which approximated the posterior distribution as N(theta_swa, Sigma_diag), which managed to pass the easy baseline without adjusting the hyperparameters. With the full SWAG approach (described in Algorithm 1 of Maddox et al 2019), which extends the variance estimation by a low-rank variance approximation obtained from the deviation matrix D_hat (storing the last K mean parameter estimates), all baselines could be passed with cost 0.829 and ECE 0.049. Also here, I kept the same hyperparameters given by the template. To further improve the predictions of the model, I implemented a learning rate decay which starts with a constant learning rate of 0.045 until epoch 10 and then switches to a linear decay with a final learning rate of 0.03. The constant phase allows the model to explore the solution space more broadly. The linear decay phase helps in fine-tuning the weights, which eventually leads to a more precise convergence to a (hopefully) global minimum. By increasing the epochs from 30 to 40 the performance could be increased. With this setup, a public cost of 0.795 and ECE of 0.067 could be reached. An additional post-hoc calibration using temperature scaling (due to the successes mentioned in papers) has been tested as well but without actual costs and ECE improvements. The reason is due to overfitting to the validation data since this data set has been used to obtain the temperature scale using LBFGS (learning rate of 0.01, max_iteration of 40). A possible option to reduce overfitting here would be to use cross-validation.